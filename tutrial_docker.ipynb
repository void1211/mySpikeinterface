{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "679e5e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pathlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pprint import pprint\n",
    "import logging\n",
    "import spikeinterface as si  # import core only\n",
    "import spikeinterface.extractors as se\n",
    "import spikeinterface.preprocessing as spre\n",
    "import spikeinterface.sorters as ss\n",
    "import spikeinterface.postprocessing as spost\n",
    "import spikeinterface.qualitymetrics as sqm\n",
    "import spikeinterface.comparison as sc\n",
    "import spikeinterface.exporters as sexp\n",
    "import spikeinterface.curation as scur\n",
    "import spikeinterface.widgets as sw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f01a5581",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "write_binary_recording \n",
      "engine=process - n_jobs=1 - samples_per_chunk=30,000 - chunk_memory=468.75 KiB - total_memory=468.75 KiB - chunk_duration=1.00s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22b843c2d515419d9f6042ae2710bbfd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "write_binary_recording (no parallelization):   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'cuda' from 'cuda' (unknown location)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m test_recording = test_recording.save(folder=\u001b[33m\"\u001b[39m\u001b[33mtest-docker-folder\u001b[39m\u001b[33m\"\u001b[39m, overwrite=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mspikeinterface\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msorters\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mrunsorter\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mrunsorter\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m sorting = \u001b[43mss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_sorter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43msorter_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mkilosort3\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrecording\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtest_recording\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfolder\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mkilosort3\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mremove_existing_folder\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdocker_image\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mspikeinterface/kilosort-compiled-base:latest\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m     12\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[38;5;28mprint\u001b[39m(sorting)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tanaka-users\\tlab\\tlab_yasui\\2025\\spikeinterface\\.venv\\Lib\\site-packages\\spikeinterface\\sorters\\runsorter.py:204\u001b[39m, in \u001b[36mrun_sorter\u001b[39m\u001b[34m(sorter_name, recording, folder, remove_existing_folder, delete_output_folder, verbose, raise_error, docker_image, singularity_image, delete_container_files, with_output, **sorter_params)\u001b[39m\n\u001b[32m    198\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m has_spython():\n\u001b[32m    199\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    200\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mThe python `spython` package must be installed to \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    201\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mrun singularity. Install with `pip install spython`\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    202\u001b[39m             )\n\u001b[32m--> \u001b[39m\u001b[32m204\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrun_sorter_container\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    205\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcontainer_image\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontainer_image\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    206\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    207\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcommon_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    208\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    210\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m run_sorter_local(**common_kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tanaka-users\\tlab\\tlab_yasui\\2025\\spikeinterface\\.venv\\Lib\\site-packages\\spikeinterface\\sorters\\runsorter.py:510\u001b[39m, in \u001b[36mrun_sorter_container\u001b[39m\u001b[34m(sorter_name, recording, mode, container_image, folder, remove_existing_folder, delete_output_folder, verbose, raise_error, with_output, delete_container_files, extra_requirements, installation_mode, spikeinterface_version, spikeinterface_folder_source, **sorter_params)\u001b[39m\n\u001b[32m    508\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m use_gpu:\n\u001b[32m    509\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m gpu_capability == \u001b[33m\"\u001b[39m\u001b[33mnvidia-required\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m510\u001b[39m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[43mhas_nvidia\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, \u001b[33m\"\u001b[39m\u001b[33mThe container requires a NVIDIA GPU capability, but it is not available\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    511\u001b[39m         extra_kwargs[\u001b[33m\"\u001b[39m\u001b[33mcontainer_requires_gpu\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    513\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m platform.system() == \u001b[33m\"\u001b[39m\u001b[33mLinux\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m has_docker_nvidia_installed():\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tanaka-users\\tlab\\tlab_yasui\\2025\\spikeinterface\\.venv\\Lib\\site-packages\\spikeinterface\\sorters\\utils\\misc.py:68\u001b[39m, in \u001b[36mhas_nvidia\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     66\u001b[39m cuda_spec = importlib.util.find_spec(\u001b[33m\"\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     67\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cuda_spec \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m68\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcuda\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m cuda\n\u001b[32m     69\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     70\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\n\u001b[32m     71\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mThis sorter requires cuda, but the package \u001b[39m\u001b[33m'\u001b[39m\u001b[33mcuda-python\u001b[39m\u001b[33m'\u001b[39m\u001b[33m is not installed. You can install it with:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mpip install cuda-python\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     72\u001b[39m     )\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'cuda' from 'cuda' (unknown location)"
     ]
    }
   ],
   "source": [
    "test_recording = si.generate_recording(num_channels=4, sampling_frequency=30000.,\n",
    "                               durations=[10.325, 3.5], set_probe=True)\n",
    "test_recording = test_recording.save(folder=\"test-docker-folder\", overwrite=True)\n",
    "\n",
    "import spikeinterface.sorters.runsorter as runsorter\n",
    "sorting = ss.run_sorter(\n",
    "    sorter_name=\"kilosort3\",\n",
    "    recording=test_recording,\n",
    "    folder=\"kilosort3\",\n",
    "    remove_existing_folder=True,\n",
    "    docker_image=\"spikeinterface/kilosort-compiled-base:latest\"\n",
    ")\n",
    "\n",
    "\n",
    "print(sorting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01bbd19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_job_kwargs = dict(n_jobs=4, chunk_duration=\"1s\")\n",
    "si.set_global_job_kwargs(**global_job_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f3aaf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = r\"C:\\Users\\tanaka-users\\tlab\\tlab_yasui\\2025\\simulations\"\n",
    "example_name = \"2025ex16\"\n",
    "condition = \"condition3\"\n",
    "dir_path = pathlib.Path(root_dir) / example_name / condition\n",
    "# binary_path = dir_path / \"signalRaw.bin\"\n",
    "# recording = se.read_binary(file_paths=binary_path, sampling_frequency=30000, num_channels=82, dtype='int16')\n",
    "# print(recording)\n",
    "\n",
    "traces = np.load(dir_path / \"signalRaw.npy\")\n",
    "traces_list = traces.T\n",
    "recording = se.NumpyRecording(\n",
    "    traces_list=[traces_list],\n",
    "    sampling_frequency=30000,\n",
    "    channel_ids=list(range(traces_list.shape[1])),\n",
    "    )\n",
    "print(recording)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa9f069",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spikeinterface.core import NumpySorting\n",
    "\n",
    "spike_times = np.load(dir_path / \"spike_times.npy\", allow_pickle=True)\n",
    "units_dict = {}\n",
    "for unit_index, times in enumerate(spike_times):\n",
    "    units_dict[unit_index] = np.array(times)\n",
    "\n",
    "gt_sorting = NumpySorting.from_unit_dict(\n",
    "    units_dict_list=units_dict,\n",
    "    sampling_frequency=30000,\n",
    ")\n",
    "print(gt_sorting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8439581",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from probeinterface import Probe\n",
    "from probeinterface.plotting import plot_probe\n",
    "\n",
    "probe_cars = dir_path / \"probe.json\"\n",
    "with open(probe_cars, 'r') as f:\n",
    "    probe_cars_dict = json.load(f)\n",
    "positions = list(zip(probe_cars_dict['x'], probe_cars_dict['y']))\n",
    "device_ids = list(range(traces_list.shape[1]))\n",
    "probe = Probe(ndim=2, si_units='um')\n",
    "probe.set_contacts(positions=positions, shapes='circle', shape_params={'radius': 5})\n",
    "probe.set_device_channel_indices(device_ids)\n",
    "print(probe)\n",
    "probe.create_auto_shape()\n",
    "plot_probe(probe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb29a07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "recording = recording.set_probe(probe)\n",
    "# recording_cmr = recording\n",
    "# recording_f = spre.bandpass_filter(recording, freq_min=300, freq_max=3000)\n",
    "# print(recording_f)\n",
    "# recording_cmr = spre.common_reference(recording_f, reference=\"global\", operator=\"median\")\n",
    "# print(recording_cmr)\n",
    "\n",
    "# # this computes and saves the recording after applying the preprocessing chain\n",
    "# recording_preprocessed = recording_cmr.save(format=\"binary\", folder=f\"preprocessed_default_{example_name}_{condition}\", overwrite=True)\n",
    "# print(recording_preprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5d0974",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"available sorters\", ss.available_sorters())\n",
    "# mountainsort4/5 はwindows非対応．\n",
    "print(\"Installed sorters\", ss.installed_sorters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286afade",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psutil\n",
    "import shutil\n",
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "from spikeinterface.sorters import run_sorter\n",
    "\n",
    "def find_and_kill_locker(file_path):\n",
    "    \"\"\"\n",
    "    指定されたファイルをロックしているプロセスを特定し、強制終了する関数。\n",
    "    psutilライブラリを使用します。\n",
    "    \"\"\"\n",
    "    target_path = Path(file_path).resolve()\n",
    "    print(f\"Searching for process locking: {target_path}\")\n",
    "\n",
    "    # すべての実行中プロセスを反復処理\n",
    "    for proc in psutil.process_iter(['pid', 'name', 'open_files']):\n",
    "        try:\n",
    "            # プロセスが開いているファイルを確認\n",
    "            if proc.info['open_files']:\n",
    "                for item in proc.info['open_files']:\n",
    "                    if Path(item.path).resolve() == target_path:\n",
    "                        print(f\"Found locking process: PID={proc.pid}, Name={proc.name}\")\n",
    "                        # 強制終了\n",
    "                        proc.kill()\n",
    "                        print(f\"Process PID={proc.pid} has been terminated.\")\n",
    "                        return True\n",
    "        except (psutil.NoSuchProcess, psutil.AccessDenied, FileNotFoundError):\n",
    "            continue\n",
    "    print(\"No process found locking the file.\")\n",
    "    return False\n",
    "\n",
    "def save_with_cleanup(recording, folder, overwrite, n_jobs, total_memory):\n",
    "    \"\"\"\n",
    "    ファイルロックエラー発生時に、ロックしているプロセスを終了させてから\n",
    "    `recording.save()`を再試行するラッパー関数。\n",
    "    \"\"\"\n",
    "    max_retries = 3\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            # save()を実行\n",
    "            return recording.save(folder=folder, overwrite=overwrite, n_jobs=n_jobs, total_memory=total_memory)\n",
    "        except PermissionError as e:\n",
    "            if attempt < max_retries - 1:\n",
    "                print(f\"PermissionError caught: {e}. Retrying after cleanup...\")\n",
    "                # ロックしているプロセスを特定して終了を試みる\n",
    "                if find_and_kill_locker(folder):\n",
    "                    # プロセス終了後、少し待機して再試行\n",
    "                    time.sleep(1)\n",
    "                    continue\n",
    "                else:\n",
    "                    # ロッカーが見つからない場合は、手動でディレクトリ削除を試みる\n",
    "                    try:\n",
    "                        shutil.rmtree(folder)\n",
    "                        print(\"Directory manually removed. Retrying save.\")\n",
    "                        time.sleep(1)\n",
    "                        continue\n",
    "                    except PermissionError as manual_e:\n",
    "                        print(f\"Manual removal failed: {manual_e}. The problem persists.\")\n",
    "                        raise manual_e\n",
    "            else:\n",
    "                print(\"Max retries reached. Failing to save.\")\n",
    "                raise e\n",
    "\n",
    "def run_sorter_with_cleanup(sorter_name, recording, folder, **kwargs):\n",
    "    \"\"\"\n",
    "    SpikeInterfaceのsorterを実行し、PermissionErrorが発生した場合は自己修復を試みる。\n",
    "    \"\"\"\n",
    "    output_folder = Path(folder) / f\"{sorter_name}_output\"\n",
    "    max_retries = 3\n",
    "    retries = 0\n",
    "\n",
    "    while retries < max_retries:\n",
    "        try:\n",
    "            print(f\"Attempting to run sorter: {sorter_name}\")\n",
    "            sorting = run_sorter(sorter_name=sorter_name, recording=recording, folder=folder,\n",
    "                                 remove_existing_folder=True, **kwargs)\n",
    "            return sorting\n",
    "        except PermissionError as e:\n",
    "            print(f\"PermissionError caught: {e}\")\n",
    "            if \"The process cannot access the file\" in str(e):\n",
    "                print(\"File is locked. Attempting to find and kill the locking process.\")\n",
    "                if find_and_kill_locker(output_folder):\n",
    "                    print(\"Locker terminated. Retrying sorter execution.\")\n",
    "                    retries += 1\n",
    "                    continue\n",
    "                else:\n",
    "                    print(\"Failed to identify and terminate a locking process. Attempting manual removal.\")\n",
    "                    try:\n",
    "                        shutil.rmtree(output_folder)\n",
    "                        print(\"Directory successfully removed. Retrying sorter execution.\")\n",
    "                        retries += 1\n",
    "                        continue\n",
    "                    except PermissionError:\n",
    "                        print(\"Manual removal failed. The problem persists.\")\n",
    "                        break\n",
    "            else:\n",
    "                raise # Re-raise if it's not the specific file lock error.\n",
    "\n",
    "    raise RuntimeError(\"Failed to run sorter after multiple retries due to persistent file lock issues.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff401985",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorters = [\"kilosort4\", \"spykingcircus2\"]\n",
    "sorting_list = []\n",
    "analyzer_list = []\n",
    "# preprocess and run sort and make analyzer\n",
    "for sorter in sorters:\n",
    "    print(\"=\"*5, sorter, \"=\"*5)\n",
    "    try:\n",
    "        if sorter == \"kilosort4\":\n",
    "            \n",
    "            pp_rec = spre.bandpass_filter(recording, freq_min=300, freq_max=3000)\n",
    "            pp_rec = spre.common_reference(pp_rec, reference=\"global\")\n",
    "            pp_rec = spre.whiten(pp_rec, int_scale=200)\n",
    "            pp_rec = spre.correct_motion(pp_rec, preset=\"kilosort_like\")\n",
    "            pp_rec = pp_rec.save(folder=f\"preprocessed_data_for_{sorter}_{example_name}_{condition}\",overwrite=True, n_jobs=8, total_memory=\"2G\")\n",
    "            # pp_rec = save_with_cleanup(pp_rec, folder=f\"preprocessed_data_for_{sorter}_{example_name}_{condition}\", overwrite=True, n_jobs=8, total_memory=\"2G\")\n",
    "            recording_preprocessed = pp_rec\n",
    "        else:\n",
    "            recording_f = spre.bandpass_filter(recording, freq_min=300, freq_max=3000)\n",
    "            recording_cmr = spre.common_reference(recording_f, reference=\"global\", operator=\"median\")\n",
    "            recording_preprocessed = recording_cmr.save(format=\"binary\", folder=f\"preprocessed_default_{example_name}_{condition}\", overwrite=True)\n",
    "        \n",
    "        sorting = ss.run_sorter(sorter_name=sorter, folder=f\"{sorter}_{example_name}_{condition}\" , remove_existing_folder=True, recording=recording_preprocessed)\n",
    "        # sorting = run_sorter_with_cleanup(sorter_name=sorter, folder=f\"{sorter}_{example_name}_{condition}\" , remove_existing_folder=True, recording=recording_preprocessed)\n",
    "        print(sorting)\n",
    "        sorting_list.append(sorting)\n",
    "\n",
    "        analyzer = si.create_sorting_analyzer(sorting=sorting, recording=recording_preprocessed, format='binary_folder', folder=f'analyzer_{sorter}_{example_name}_{condition}', overwrite=True)\n",
    "        print(analyzer)\n",
    "        analyzer_list.append(analyzer)\n",
    "    finally:\n",
    "        if 'pp_rec' in locals():\n",
    "            del pp_rec\n",
    "        if 'recording_preprocessed' in locals():\n",
    "            del recording_preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f66c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "extensions_to_compute = [\n",
    "    \"random_spikes\",\n",
    "    \"waveforms\",\n",
    "    \"noise_levels\",\n",
    "    \"templates\",\n",
    "    \"spike_amplitudes\",\n",
    "    \"unit_locations\",\n",
    "    \"spike_locations\",\n",
    "    \"correlograms\",\n",
    "    \"template_similarity\"\n",
    "]\n",
    "\n",
    "extension_params = {\n",
    "    \"unit_locations\": {\"method\": \"center_of_mass\"},\n",
    "    \"spike_locations\": {\"ms_before\": 0.1},\n",
    "    \"correlograms\": {\"bin_ms\": 0.1},\n",
    "    \"template_similarity\": {\"method\": \"cosine_similarity\"}\n",
    "}\n",
    "for analyzer_index, analyzer in enumerate(analyzer_list):\n",
    "    print(\"=\"*5, sorters[analyzer_index], \"=\"*5)\n",
    "    analyzer.compute(extensions_to_compute, extension_params=extension_params)\n",
    "    del analyzer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8164640e",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(analyzer_list)\n",
    "n_unit = 3\n",
    "fig, ax = plt.subplots(n,n_unit)\n",
    "for analyzer_index, analyzer in enumerate(analyzer_list):\n",
    "    print(\"=\"*5, sorters[analyzer_index], \"=\"*5)\n",
    "    for unit_index, unit_id in enumerate(analyzer.unit_ids[:n_unit]):\n",
    "        template = analyzer.get_extension(\"templates\").get_data(operator=\"average\")[unit_index]\n",
    "        ax[analyzer_index,unit_index].plot(template)\n",
    "        ax[analyzer_index,unit_index].set_title(f\"{sorters[analyzer_index]}: unit{unit_id}\")\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a437c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_gt_list = []\n",
    "for sorting_index, sorting in enumerate(sorting_list):\n",
    "    print(\"=\"*5, sorters[sorting_index], \"=\"*5)\n",
    "    comp_gt = sc.compare_sorter_to_ground_truth(\n",
    "        gt_sorting=gt_sorting, \n",
    "        tested_sorting=sorting, \n",
    "        agreement_method=\"count\",\n",
    "        )\n",
    "    comp_gt_list.append(comp_gt)\n",
    "    print(comp_gt)\n",
    "# comp_pair = sc.compare_two_sorters(sorting1=sorting_KS4, sorting2=sorting_SC2)\n",
    "# comp_multi = sc.compare_multiple_sorters(\n",
    "#     sorting_list=[sorting_TDC, sorting_SC2, sorting_KS2], name_list=[\"tdc\", \"sc2\", \"ks2\"]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a686522",
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_unit_positions = np.load(dir_path / \"cell_positions.npy\")\n",
    "plot_probe(probe)\n",
    "plt.scatter(gt_unit_positions[:,0], gt_unit_positions[:,1], c=\"red\", s=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529ebcd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import List\n",
    "\n",
    "def compare_spike_times(gt_sorting, tested_sorting, delta_time_ms: float, fs: float, mode: str = \"rate\") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Compares two lists of spike times and builds a simple agreement matrix.\n",
    "    This function allows for one-to-many matching.\n",
    "\n",
    "    Args:\n",
    "        gt_sorting: The Ground Truth sorting object.\n",
    "        tested_sorting: The tested sorting object.\n",
    "        delta_time_ms (float): The time tolerance for matching, in milliseconds.\n",
    "        fs (float): The sampling frequency in Hz.\n",
    "        mode (str): The mode to calculate matrix. \"rate\" or \"count\".\n",
    "    Returns:\n",
    "        np.ndarray: An agreement matrix where rows correspond to GT units and columns correspond to tested units. The values are the number of matching spikes.\n",
    "    \"\"\"\n",
    "    # Convert delta_time from ms to frames\n",
    "    delta_time_frames = delta_time_ms * fs / 1000\n",
    "\n",
    "    gt_unit_ids = gt_sorting.get_unit_ids()\n",
    "    gt_spike_times = {unit_id: gt_sorting.get_unit_spike_train(unit_id) for unit_id in gt_unit_ids}\n",
    "    tested_unit_ids = tested_sorting.get_unit_ids()\n",
    "    tested_spike_times = {unit_id: tested_sorting.get_unit_spike_train(unit_id) for unit_id in tested_unit_ids}\n",
    "    n_gt = len(gt_unit_ids)\n",
    "    n_tested = len(tested_unit_ids)\n",
    "    TP_matrix = np.zeros((n_gt, n_tested), dtype=int)\n",
    "    FP_matrix = np.zeros((n_gt, n_tested), dtype=int)\n",
    "    FN_matrix = np.zeros((n_gt, n_tested), dtype=int)\n",
    "\n",
    "    # Correct iteration: Iterate through the individual spike time arrays\n",
    "    for i, gt_id in enumerate(gt_unit_ids):\n",
    "        gt_unit_spikes = gt_spike_times[gt_id]\n",
    "        if gt_unit_spikes.size == 0:\n",
    "            continue\n",
    "        \n",
    "        for j, tested_id in enumerate(tested_unit_ids):\n",
    "            tested_unit_spikes = tested_spike_times[tested_id]\n",
    "            if tested_unit_spikes.size == 0:\n",
    "                continue\n",
    "\n",
    "            matches = 0\n",
    "            \n",
    "            # The original code had a bug here.\n",
    "            # It was iterating over a list of arrays, not a single array.\n",
    "            \n",
    "            # Correct logic: for each tested spike, find a match in the GT unit's array\n",
    "            for gt_spike_time in gt_unit_spikes:\n",
    "                # np.searchsorted must be used on a single, 1D array\n",
    "                idx = np.searchsorted(tested_unit_spikes, gt_spike_time)\n",
    "                \n",
    "                found_match = False\n",
    "                if idx < len(tested_unit_spikes) and np.abs(tested_unit_spikes[idx] - gt_spike_time) <= delta_time_frames:\n",
    "                    found_match = True\n",
    "                elif idx > 0 and np.abs(tested_unit_spikes[idx-1] - gt_spike_time) <= delta_time_frames:\n",
    "                    found_match = True\n",
    "                \n",
    "                if found_match:\n",
    "                    matches += 1\n",
    "                    \n",
    "            TP_matrix[i, j] = matches if mode == \"count\" else matches / len(gt_unit_spikes) * 100\n",
    "            FP_matrix[i, j] = len(tested_unit_spikes) - matches if mode == \"count\" else ((len(tested_unit_spikes) - matches) / len(tested_unit_spikes) * 100)\n",
    "            FN_matrix[i, j] = len(gt_unit_spikes) - matches if mode == \"count\" else ((len(gt_unit_spikes) - matches) / len(gt_unit_spikes) * 100)\n",
    "            \n",
    "    return TP_matrix, FP_matrix, FN_matrix\n",
    "\n",
    "def plot_matrix(matrix, title, gt_unit_num, tested_unit_num, ax):\n",
    "    im = ax.imshow(matrix, cmap=\"Blues\")\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(\"Tested units\")\n",
    "    ax.set_ylabel(\"GT units\")\n",
    "    ax.set_xticks(range(tested_unit_num))\n",
    "    ax.set_yticks(range(gt_unit_num))\n",
    "    \n",
    "    # 各セルに値を書き込む\n",
    "    for i in range(gt_unit_num):\n",
    "        for j in range(tested_unit_num):\n",
    "            text = ax.text(j, i, f'{matrix[i, j]}',\n",
    "                         ha=\"center\", va=\"center\", color=\"red\", fontsize=8)\n",
    "\n",
    "    # Example usage\n",
    "# You need to specify a sampling frequency (fs) and a jitter threshold (delta_time_ms)\n",
    "sampling_frequency = 30000.0  # Hz\n",
    "time_threshold_ms = 1.0      # milliseconds\n",
    "\n",
    "# Calculate the agreement matrix\n",
    "for i, sorting in enumerate(sorting_list):\n",
    "    TP_matrix, FP_matrix, FN_matrix = compare_spike_times(\n",
    "        gt_sorting=gt_sorting,\n",
    "        tested_sorting=sorting,\n",
    "        delta_time_ms=time_threshold_ms,\n",
    "        fs=sampling_frequency,\n",
    "        mode=\"rate\"\n",
    "    )\n",
    "\n",
    "    if len(gt_sorting.get_unit_ids()) > len(sorting_list[i].get_unit_ids()):\n",
    "        fig, ax = plt.subplots(1, 3, figsize=(30,24))\n",
    "    else:\n",
    "        fig, ax = plt.subplots(3, 1, figsize=(24,30))\n",
    "    plot_matrix(TP_matrix, \"TP\", len(gt_sorting.get_unit_ids()), len(sorting_list[i].get_unit_ids()), ax[0])\n",
    "    plot_matrix(FP_matrix, \"FP\", len(gt_sorting.get_unit_ids()), len(sorting_list[i].get_unit_ids()), ax[1])\n",
    "    plot_matrix(FN_matrix, \"FN\", len(gt_sorting.get_unit_ids()), len(sorting_list[i].get_unit_ids()), ax[2])\n",
    "    # fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f5e7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_isi(spike_times):\n",
    "    isi = np.diff(spike_times)\n",
    "    return isi\n",
    "\n",
    "def get_isi_violation_rate(spike_times, threshold_ms=2.0, sampling_frequency=30000.0):\n",
    "    isi = get_isi(spike_times)\n",
    "    isi_violation = np.sum(isi < threshold_ms * sampling_frequency / 1000) / len(isi) * 100\n",
    "    return isi_violation\n",
    "\n",
    "def get_isi_violation_rate_by_unit(sorting, threshold_ms=2.0, sampling_frequency=30000.0, dtype=\"dict\"):\n",
    "    isi_violation = {}\n",
    "    for unit_id in sorting.get_unit_ids():\n",
    "        isi_violation[unit_id] = get_isi_violation_rate(sorting.get_unit_spike_train(unit_id), threshold_ms, sampling_frequency)\n",
    "    if dtype == \"dict\":\n",
    "        return isi_violation\n",
    "    elif dtype == \"list\":\n",
    "        return list(isi_violation.values())\n",
    "    else:\n",
    "        ValueError(\"dtype must be 'dict' or 'list'\")\n",
    "\n",
    "def get_single_multi_unit(sorting, threshold_rate=50, threshold_ms=2.0, sampling_frequency=30000.0, unit_type=\"both\"):\n",
    "    single_unit = []\n",
    "    multi_unit = []\n",
    "    isi_violation = get_isi_violation_rate_by_unit(sorting, threshold_ms, sampling_frequency)\n",
    "    unit_ids = sorting.get_unit_ids()\n",
    "    for i, unit_id in enumerate(unit_ids):\n",
    "        if isi_violation[unit_id] > threshold_rate:\n",
    "            multi_unit.append(int(unit_id))\n",
    "        else:\n",
    "            single_unit.append(int(unit_id))\n",
    "\n",
    "    try:    \n",
    "        if unit_type == \"single\":\n",
    "            return single_unit\n",
    "        elif unit_type == \"multi\":\n",
    "            return multi_unit\n",
    "        elif unit_type == \"both\":\n",
    "            return single_unit, multi_unit\n",
    "    except:\n",
    "        ValueError(\"unit_type must be 'single', 'multi', or 'both'\")\n",
    "    \n",
    "        \n",
    "\n",
    "sampling_frequency = 30000.0  # Hz\n",
    "time_threshold_ms = 1.0      # milliseconds\n",
    "mua_threshold_rate = 0.2\n",
    "for i, sorting in enumerate(sorting_list):\n",
    "    isi_violation_dict = get_isi_violation_rate_by_unit(sorting)\n",
    "    isi_violation_values = list(isi_violation_dict.values())\n",
    "    plt.plot(isi_violation_values, label=sorters[i])\n",
    "    print(\"=\"*5, f\"{sorters[i]}\", \"=\"*5, f\"\\n single: {get_single_multi_unit(sorting, threshold_rate=mua_threshold_rate, unit_type=\"single\")}, \\n multi: {get_single_multi_unit(sorting, threshold_rate=mua_threshold_rate, unit_type=\"multi\")}\")\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f6906a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 散布図行列を作成する関数\n",
    "def create_scatter_matrix(sorting_list, sorters, metrics_func, metric_name, figsize=(12, 8)):\n",
    "    \"\"\"\n",
    "    複数のソーターの結果を比較する散布図行列を作成\n",
    "    \n",
    "    Args:\n",
    "        sorting_list: ソーターの結果のリスト\n",
    "        sorters: ソーター名のリスト\n",
    "        metrics_func: メトリクスを計算する関数\n",
    "        metric_name: メトリクスの名前\n",
    "        figsize: 図のサイズ\n",
    "    \"\"\"\n",
    "    n_sorters = len(sorting_list)\n",
    "    \n",
    "    # 各ソーターのメトリクスを計算\n",
    "    metrics_data = {}\n",
    "    for i, sorting in enumerate(sorting_list):\n",
    "        metrics_data[sorters[i]] = metrics_func(sorting)\n",
    "    \n",
    "    # 散布図行列を作成\n",
    "    fig, axes = plt.subplots(n_sorters, n_sorters, figsize=figsize)\n",
    "    \n",
    "    # axesの配列構造を統一\n",
    "    if n_sorters == 1:\n",
    "        axes = np.array([[axes]])\n",
    "    elif n_sorters == 2:\n",
    "        axes = axes.reshape(2, 2)\n",
    "    \n",
    "    for i in range(n_sorters):\n",
    "        for j in range(n_sorters):\n",
    "            ax = axes[i, j]\n",
    "            \n",
    "            if i == j:\n",
    "                # 対角線：ヒストグラム\n",
    "                values = list(metrics_data[sorters[i]].values())\n",
    "                ax.hist(values, bins=20, alpha=0.7, edgecolor='black')\n",
    "                ax.set_title(f'{sorters[i]}\\n{metric_name} Distribution')\n",
    "                ax.set_xlabel(metric_name)\n",
    "                ax.set_ylabel('Count')\n",
    "            else:\n",
    "                # 非対角線：散布図\n",
    "                sorter1 = sorters[i]\n",
    "                sorter2 = sorters[j]\n",
    "                \n",
    "                # 共通のユニットIDを取得\n",
    "                common_units = set(metrics_data[sorter1].keys()) & set(metrics_data[sorter2].keys())\n",
    "                \n",
    "                if common_units:\n",
    "                    x_values = [metrics_data[sorter1][unit_id] for unit_id in common_units]\n",
    "                    y_values = [metrics_data[sorter2][unit_id] for unit_id in common_units]\n",
    "                    \n",
    "                    ax.scatter(x_values, y_values, alpha=0.6, s=50)\n",
    "                    \n",
    "                    # 相関係数を計算\n",
    "                    if len(x_values) > 1:\n",
    "                        correlation = np.corrcoef(x_values, y_values)[0, 1]\n",
    "                        ax.text(0.05, 0.95, f'r = {correlation:.3f}', \n",
    "                               transform=ax.transAxes, verticalalignment='top',\n",
    "                               bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "                    \n",
    "                    ax.set_xlabel(f'{sorter1} {metric_name}')\n",
    "                    ax.set_ylabel(f'{sorter2} {metric_name}')\n",
    "                else:\n",
    "                    ax.text(0.5, 0.5, 'No common units', \n",
    "                           transform=ax.transAxes, ha='center', va='center')\n",
    "                    ax.set_xlabel(f'{sorter1} {metric_name}')\n",
    "                    ax.set_ylabel(f'{sorter2} {metric_name}')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "# ISI違反率の散布図行列を作成\n",
    "print(\"ISI Violation Rate Scatter Matrix:\")\n",
    "fig1 = create_scatter_matrix(\n",
    "    sorting_list, \n",
    "    sorters, \n",
    "    get_isi_violation_by_unit, \n",
    "    \"ISI Violation Rate\",\n",
    "    figsize=(10, 8)\n",
    ")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623b435c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ユニット数とスパイク数の散布図行列を作成\n",
    "def get_unit_spike_counts(sorting):\n",
    "    \"\"\"各ユニットのスパイク数を取得\"\"\"\n",
    "    spike_counts = {}\n",
    "    for unit_id in sorting.get_unit_ids():\n",
    "        spike_train = sorting.get_unit_spike_train(unit_id)\n",
    "        spike_counts[unit_id] = len(spike_train)\n",
    "    return spike_counts\n",
    "\n",
    "def get_unit_firing_rates(sorting, sampling_frequency=30000.0):\n",
    "    \"\"\"各ユニットの発火率を取得（Hz）\"\"\"\n",
    "    firing_rates = {}\n",
    "    for unit_id in sorting.get_unit_ids():\n",
    "        spike_train = sorting.get_unit_spike_train(unit_id)\n",
    "        if len(spike_train) > 0:\n",
    "            # レコーディングの長さを取得（サンプル数）\n",
    "            recording_duration_samples = sorting.get_total_samples()\n",
    "            recording_duration_seconds = recording_duration_samples / sampling_frequency\n",
    "            firing_rates[unit_id] = len(spike_train) / recording_duration_seconds\n",
    "        else:\n",
    "            firing_rates[unit_id] = 0.0\n",
    "    return firing_rates\n",
    "\n",
    "# スパイク数の散布図行列\n",
    "print(\"Spike Count Scatter Matrix:\")\n",
    "fig2 = create_scatter_matrix(\n",
    "    sorting_list, \n",
    "    sorters, \n",
    "    get_unit_spike_counts, \n",
    "    \"Spike Count\",\n",
    "    figsize=(10, 8)\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "# 発火率の散布図行列\n",
    "print(\"Firing Rate Scatter Matrix:\")\n",
    "fig3 = create_scatter_matrix(\n",
    "    sorting_list, \n",
    "    sorters, \n",
    "    get_unit_firing_rates, \n",
    "    \"Firing Rate (Hz)\",\n",
    "    figsize=(10, 8)\n",
    ")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93869090",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spike_num_by_unit(sorting, dtype=\"dict\"):\n",
    "    spike_num = {}\n",
    "    for unit_id in sorting.get_unit_ids():\n",
    "        spike_num[unit_id] = len(sorting.get_unit_spike_train(unit_id))\n",
    "    if dtype == \"dict\":\n",
    "        return spike_num\n",
    "    elif dtype == \"list\":\n",
    "        return list(spike_num.values())\n",
    "    else:\n",
    "        ValueError(\"dtype must be 'dict' or 'list'\")\n",
    "\n",
    "for i, sorting in enumerate(sorting_list):\n",
    "    isi_violation_list = get_isi_violation_rate_by_unit(sorting, dtype=\"list\")\n",
    "    spike_num_list = get_spike_num_by_unit(sorting, dtype=\"list\")\n",
    "    plt.scatter(spike_num_list, isi_violation_list, label=sorters[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74028137",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, comp_gt in enumerate(comp_gt_list):\n",
    "#     print(\"=\"*5, sorters[i], \"=\"*5)\n",
    "#     print(comp_gt.get_performance(method=\"by_unit\"))\n",
    "#     if len(gt_sorting.get_unit_ids()) > len(sorting_list[i].get_unit_ids()):\n",
    "#         fig, ax = plt.subplots(1, 2, figsize=(10,8))\n",
    "#     else:\n",
    "#         fig, ax = plt.subplots(2, 1, figsize=(10,8))\n",
    "        \n",
    "#     # 混同行列\n",
    "#     sw.plot_confusion_matrix(\n",
    "#         comp_gt,\n",
    "#         unit_ticks=True,\n",
    "#         count_text=True,\n",
    "#         ax=ax[0], \n",
    "#     )\n",
    "\n",
    "#     # 合意行列\n",
    "#     w_agr = sw.plot_agreement_matrix(\n",
    "#         comp_gt, \n",
    "#         ordered=False, \n",
    "#         count_text=True, \n",
    "#         unit_ticks=False, \n",
    "#         ax=ax[1],\n",
    "#     )\n",
    "#     ax[0].set_title(f\"{sorters[i]} confusion\")\n",
    "#     ax[1].set_title(f\"{sorters[i]} agreement\")\n",
    "#     fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5e0485",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sw.plot_all_amplitudes_distributions(analyzer_list[0])\n",
    "# sw.plot_amplitudes(analyzer_list[0])\n",
    "# sw.plot_autocorrelograms(analyzer_list[0])\n",
    "# # sw.plot_comparison_collision_by_similarity(\n",
    "# #     comp_gt_list[0], \n",
    "# #     templates_array=np.load(dir_path / \"spike_templates.npy\", allow_pickle=True).T,\n",
    "# #     )\n",
    "# sw.plot_crosscorrelograms(analyzer_list[0])\n",
    "# sw.plot_isi_distribution(analyzer_list[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19c19ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sw.plot_multicomparison_agreement(comp_gt_list[0])\n",
    "# sw.plot_multicomparison_agreement_by_sorter(comp_gt_list[0])\n",
    "# sw.plot_multicomparison_graph(comp_gt_list[0])\n",
    "# sw.plot_peak_activity(recording, analyzer_list[0].get_detect_peaks())\n",
    "# sw.plot_probe_map(recording)\n",
    "# # sw.plot_quality_metrics(analyzer_list[0])\n",
    "# sw.plot_rasters(analyzer_list[0])\n",
    "\n",
    "# sw.plot_sorting_summary(analyzer_list[0], backend=\"spikeinterface_gui\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
